import asyncio
import os

from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.messages import BaseChatMessage
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from docx import Document
from markdownify import markdownify as md

from utils.model_loader import get_model_client

# === llm client ===
model_client = get_model_client(provider="ollama", model="llama3.2")

DEFAULT_INPUT_PATH = "/Users/ancient/data/autogen/input.docx"
DEFAULT_PROMPT_PATH = "/Users/ancient/data/autogen/prompt.txt"
DEFAULT_OUTPUT_PATH = "/Users/ancient/data/autogen/output.md"


# === å·¥å…·å‡½æ•° ===
def docx_to_markdown(docx_path):
    doc = Document(docx_path)
    text = "\n".join([p.text for p in doc.paragraphs])
    return md(text)


def load_prompt(prompt_file=None):
    if prompt_file and os.path.exists(prompt_file):
        with open(prompt_file, "r", encoding="utf-8") as f:
            return f.read()
    else:
        return input("è¯·è¾“å…¥æç¤ºè¯ï¼š")


def save_markdown(output_text, output_path):
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(output_text)
    print(f"âœ… æ‰©å†™ç»“æœå·²ä¿å­˜è‡³ {output_path}")


async def log_and_save_stream(stream):
    history = []
    last_response = None

    async for event in stream:
        if getattr(event, "type", None) == "TextMessage":
            print(f"ğŸ—£ï¸ {event.source}: {event.content}")
            history.append({"sender": event.source, "content": event.content})

            if event.source == "assistant":
                last_response = event.content

    return last_response, history


async def main(docx_path, prompt_path, output_path):
    prompt = load_prompt(prompt_path)
    raw_markdown = docx_to_markdown(docx_path)

    print("\nğŸ“„ åŸå§‹è¯­æ–™ï¼ˆå·²è½¬ Markdownï¼‰ï¼š\n")
    print(raw_markdown[:500] + "..." if len(raw_markdown) > 500 else raw_markdown)

    task = f"""ä»¥ä¸‹æ˜¯æˆ‘çš„æç¤ºè¯ï¼š
    {prompt}

    ä»¥ä¸‹æ˜¯åŸå§‹è¯­æ–™ï¼ˆMarkdown æ ¼å¼ï¼‰ï¼š
    {raw_markdown}

    è¯·åŸºäºæç¤ºè¯ï¼Œå¯¹è¯­æ–™è¿›è¡Œå†…å®¹ä¸°å¯Œã€é£æ ¼ä¸€è‡´çš„æ‰©å†™ï¼Œå°½å¯èƒ½æå‡å†…å®¹æ·±åº¦å’Œè¡¨è¾¾è´¨é‡ã€‚"""

    # === Agent + Client ===
    assistant = AssistantAgent("assistant", model_client=model_client)

    user_proxy = UserProxyAgent("user_proxy", input_func=input)  # Consoleè¾“å…¥
    termination = TextMentionTermination("APPROVE")  # ä½ æ‰‹åŠ¨è¾“å…¥ APPROVE å³ç»“æŸ

    team = RoundRobinGroupChat(
        [assistant, user_proxy], termination_condition=termination
    )

    stream = team.run_stream(task=task)

    last_response, _history = await log_and_save_stream(stream)

    # await Console(stream)  # æ§åˆ¶å°æµè¾“å‡º
    await model_client.close()

    if last_response:
        save_markdown(last_response, output_path)
    else:
        print("âŒ æ²¡æœ‰æ•è·åˆ° Assistant çš„è¾“å‡ºå†…å®¹ã€‚")


# === CLI è°ƒç”¨ç¤ºä¾‹ ===
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Autogen LLM å†™ä½œæ‰©å†™å·¥ä½œæµ")

    parser.add_argument(
        "--file",
        type=str,
        default=DEFAULT_INPUT_PATH,
        help="ä¸Šä¼ çš„ Word æ–‡ä»¶è·¯å¾„ï¼ˆ.docxï¼‰",
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default=DEFAULT_PROMPT_PATH,
        help="æç¤ºè¯æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼Œå¯é€‰",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=DEFAULT_OUTPUT_PATH,
        help="è¾“å‡º Markdown æ–‡ä»¶è·¯å¾„",
    )

    args = parser.parse_args()
    asyncio.run(main(args.file, args.prompt, args.output))
