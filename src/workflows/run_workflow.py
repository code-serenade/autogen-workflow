import asyncio
import json
import os

from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.messages import BaseChatMessage
from autogen_agentchat.teams import RoundRobinGroupChat

from utils.model_loader import get_model_client

# === é»˜è®¤è·¯å¾„ ===
DEFAULT_INPUT_PATH = "data/input.jsonl"
DEFAULT_PROMPT_PATH = "data/prompt.txt"
DEFAULT_OUTPUT_MD = "data/output/output.md"
DEFAULT_OUTPUT_JSONL = "data/output/output.jsonl"

# === å·¥å…·å‡½æ•° ===


def load_prompt(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    return input("è¯·è¾“å…¥æç¤ºè¯ï¼š")


def load_jsonl(path):
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f if line.strip()]


def append_markdown(content, output_path, item_id=None):
    with open(output_path, "a", encoding="utf-8") as f:
        if item_id is not None:
            f.write(f"\n\n## æ‰©å†™ç»“æœ {item_id}\n")
        f.write(content + "\n")
        f.write("\n---\n")
    print(f"âœ… ç¬¬ {item_id} æ¡å†…å®¹å·²è¿½åŠ ä¿å­˜åˆ° {output_path}")


def append_jsonl(data: dict, path: str):
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(data, ensure_ascii=False) + "\n")


async def log_and_save_stream(stream, item_id, md_path, jsonl_path):
    last_response = None
    async for event in stream:
        if getattr(event, "type", None) == "TextMessage":
            print(f"ğŸ—£ï¸ {event.source}: {event.content}")
            if event.source == "assistant":
                last_response = event.content

    if last_response:
        append_markdown(last_response, md_path, item_id=item_id)
        append_jsonl({"id": item_id, "output": last_response}, jsonl_path)
    else:
        print(f"âŒ ç¬¬ {item_id} æ¡æ•°æ®æœªç”Ÿæˆå†…å®¹")


# === ä¸»é€»è¾‘ ===
async def main(input_path, prompt_path, output_md, output_jsonl):
    model_client = get_model_client(provider="ollama", model="llama3.2")
    prompt = load_prompt(prompt_path)
    items = load_jsonl(input_path)

    for idx, item in enumerate(items):
        item_id = item.get("id", idx + 1)
        print(f"\nğŸš€ æ­£åœ¨å¤„ç†ç¬¬ {idx + 1} æ¡æ•°æ®ï¼ˆID: {item_id}ï¼‰...\n")

        task = f"""ä»¥ä¸‹æ˜¯æˆ‘çš„æç¤ºè¯ï¼š
{prompt}

ä»¥ä¸‹æ˜¯åŸå§‹è¯­æ–™ï¼š
{item['text']}

è¯·åŸºäºæç¤ºè¯ï¼Œå¯¹è¯­æ–™è¿›è¡Œå†…å®¹ä¸°å¯Œã€é£æ ¼ä¸€è‡´çš„æ‰©å†™ï¼Œå°½å¯èƒ½æå‡å†…å®¹æ·±åº¦å’Œè¡¨è¾¾è´¨é‡ã€‚"""

        assistant = AssistantAgent("assistant", model_client=model_client)
        user_proxy = UserProxyAgent("user", input_func=input)
        termination = TextMentionTermination("APPROVE")

        team = RoundRobinGroupChat(
            [assistant, user_proxy], termination_condition=termination
        )
        stream = team.run_stream(task=task)

        await log_and_save_stream(
            stream, item_id=item_id, md_path=output_md, jsonl_path=output_jsonl
        )

    await model_client.close()
    print("ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæ¯•ï¼")


# === CLI ===
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Autogen å¤šæ¡è¯­æ–™æ‰©å†™å·¥ä½œæµ")

    parser.add_argument(
        "--input", type=str, default=DEFAULT_INPUT_PATH, help="è¾“å…¥ .jsonl æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--prompt", type=str, default=DEFAULT_PROMPT_PATH, help="æç¤ºè¯æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output-md", type=str, default=DEFAULT_OUTPUT_MD, help="Markdown è¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output-jsonl",
        type=str,
        default=DEFAULT_OUTPUT_JSONL,
        help="ç»“æ„åŒ–è¾“å‡º jsonl è·¯å¾„",
    )

    args = parser.parse_args()

    asyncio.run(main(args.input, args.prompt, args.output_md, args.output_jsonl))
